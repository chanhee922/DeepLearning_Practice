{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "다층신경망 코딩을 통한 이해!!.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOig84N42RKDGqyWNHp4wxd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chanhee922/DeepLearning_Practice/blob/master/%EB%8B%A4%EC%B8%B5%EC%8B%A0%EA%B2%BD%EB%A7%9D_%EC%BD%94%EB%94%A9%EC%9D%84_%ED%86%B5%ED%95%9C_%EC%9D%B4%ED%95%B4!!.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vQGD-Nw8Gqw0",
        "colab_type": "text"
      },
      "source": [
        "# 2개층을 가진 다층신경망\n",
        "## (입력층 1개, 은닉층 1개, 출력층 1개)\n",
        "\n",
        "### - 입력층 shape: 입력 특성의 개수\n",
        "### - 은닉층 shape: hyper-parameter(뉴런의 개수는 직접 결정)\n",
        "### - 출력층 shape: 출력층의 노드 개수 = 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C3qPJGWa0M8Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s2gBECP-z9Sq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# SingleLayer class (단층신경망구현)에 배치입력을 통한, \n",
        "# 배치경사하강법(Batch Gradient Descent) 적용\n",
        "class SingleLayer:\n",
        "    \n",
        "    def __init__(self, learning_rate=.1, l1=0, l2=0): # 학습률(에타) 하이퍼파라미터 추가\n",
        "        self.w = None\n",
        "        self.b = None        \n",
        "        \n",
        "        # 손실함수의 평균오차값을 매 훈련시마다(epoch) 저장\n",
        "        self.losses = []       # 훈련셋의 손실값을 저장하는 리스트\n",
        "        self.val_losses = []   # 검증셋의 손실값을 저장하는 리스트\n",
        "        \n",
        "        self.w_history = []      # NEW: 매훈련시마다, 학습으로 변하는 가중치 저장\n",
        "        \n",
        "        self.lr   = learning_rate  # NEW: 학습률 저장\n",
        "        \n",
        "        # L1/L2 규제강도를 조절하는 alpha 값 저장\n",
        "        self.l1 = l1    # L1 norm\n",
        "        self.l2 = l2    # L2 norm\n",
        "        \n",
        "        pass # constructor\n",
        "    \n",
        "    \n",
        "    # 정방향계산(=뉴런의 선형함수 계산)\n",
        "    def forpass(self, x):  \n",
        "        # X : 2-D, Matrix\n",
        "        # self.w : 2-D, Matrix\n",
        "        # z = w1x1 + w2x2 + ..... wnxn + b\n",
        "        \n",
        "#         z = np.sum(x * self.w) + self.b\n",
        "        z = np.dot(x, self.w) + self.b\n",
        "        return z\n",
        "        \n",
        "        pass # forpass\n",
        "    \n",
        "    \n",
        "    # 역방향계산(오차역전파와 손실함수의 미분을 통한 가중치와 절편의 변화율 계산)\n",
        "    def backprop(self, x, err):\n",
        "        m = len(x)   # 전체샘플크기(m)를 구합니다.\n",
        "        \n",
        "#         w_grad = x * err    \n",
        "#         b_grad = 1 * err    \n",
        "        \n",
        "        # 가중치(w)에 대한 변화율(=gradient) \"평균\" 계산\n",
        "        w_grad = np.dot(x.T, err) / m  # m: 전체샘플크기       \n",
        "        \n",
        "        # 절편(b)에 대한 변화율(=gradient) \"평균\" 계산\n",
        "        b_grad = np.sum(err) / m   #  m: 전체샘플크기    \n",
        "    \n",
        "        return w_grad, b_grad\n",
        "        pass # backprop\n",
        "    \n",
        "    \n",
        "    # 활성화함수(시그모이드) 통과하여 반환\n",
        "    def activation(self, z):\n",
        "        # **** 안전한 지수(또는 로그) 계산을 위해, 지정됨 범위 값을 가지도록 함\n",
        "#         z = np.clip(z, 1e-10, 1 - 1e-10)   # 1st. method\n",
        "        z = np.clip(z, -100, None)         # 2nd. method\n",
        "    \n",
        "        a = 1 / (1 + np.exp(-z))\n",
        "        return a\n",
        "        pass # activation\n",
        "    \n",
        "    \n",
        "    # 신경망 훈련을 통한 최적의 가중치(w)와 절편(b)을\n",
        "    # 찾아가도록 훈련\n",
        "    def fit(self, x, y, epochs=100, x_val=None, y_val=None):\n",
        "        m = len(y)\n",
        "        \n",
        "        # (0). 정답을 열벡터 형태의 모양으로 reshaping\n",
        "        y = y.reshape(-1, 1)         # 훈련셋의 정답\n",
        "        y_val = y_val.reshape(-1, 1) # 검증셋의 정답                     \n",
        "        \n",
        "        # (1). 가중치와 절편의 초기값 설정\n",
        "        # 2-D, Matrix with all elements is 1\n",
        "        self.w = np.ones( (x.shape[1], 1) )  \n",
        "        self.b = 0\n",
        "        \n",
        "        # 초기 가중치 값 리스트에 추가\n",
        "        self.w_history.append(self.w.copy()) \n",
        "        \n",
        "        # (2)  epoch 돌립니다.\n",
        "        for i in range(epochs):\n",
        "            \n",
        "            # (3) 전체샘플을 활용한 학습(가중치와 절편을 업데이트) 수행\n",
        "            #\n",
        "            # 신경망을 훈련시킬 훈련 데이터 셋은, 있는 그대로 밀어넣는게 아니라,\n",
        "            # 매 에포크(epoch) 마다, shuffling해서, 밀어 넣어야 함!!!\n",
        "            # 전체 sample 을 무작위로 섞는 shuffling 함수 ==> np.random.permutation()함수\n",
        "            \n",
        "            indexes = np.arange( len(x) )  # 전체 샘플의 index 번호를 1-D, Vector로 생성\n",
        "            indexes = np.random.permutation( indexes  ) # shuffling 후 다시 저장\n",
        "            \n",
        "            # 매 epoch 마다 평균 손실함수의 오차값 산출하여, 리스트에 저장\n",
        "            loss = 0\n",
        "            \n",
        "            # 안쪽의 매 샘플마다 반복시키는 loop는 삭제!!!\n",
        "            # 왜? 한번입력이 전체 샘플이기 때문에....\n",
        "            \n",
        "#             xi = x[i]   # 매번 샘플의 입력데이타\n",
        "#             yi = y[i]   # 매번 샘플의 정답데이타\n",
        "\n",
        "            # (4) 정방향계산 수행\n",
        "            z = self.forpass(x)\n",
        "\n",
        "            # (5) 활성화함수 통과\n",
        "            a = self.activation(z)\n",
        "\n",
        "            # (6) 오차계산\n",
        "            err = -(y - a)\n",
        "\n",
        "            # (7) 오차역전파를 통한, 가중치와 절편의 변화율 계산\n",
        "            w_grad, b_grad = self.backprop(x, err)\n",
        "\n",
        "            # (***) L1/L2 \" 평균\"규제를 적용\n",
        "            w_grad += ( self.l1 * np.sign(self.w) + self.l2 * self.w ) / m\n",
        "\n",
        "            # (8) 계산된 가중치와 절편의 변화율을 이용한, 가중치/절편의 업데이트\n",
        "            # NEW: 가중치의 업데이트 양을 조절하는 하이퍼파라미터인 \"학습률(에타)\" 적용\n",
        "            self.w -= self.lr * w_grad  \n",
        "            self.b -= b_grad\n",
        "\n",
        "            # NEW: 매번 업데이트 되는 가중치를 파이썬 리스트에추가\n",
        "            self.w_history.append(self.w.copy()) # Deep copy\n",
        "\n",
        "            # (9) 로지스틱 손실함수의 계산 수행\n",
        "            # 안전한 로그계산을 위한 클리핑 수행\n",
        "            a = np.clip(a, 1e-10, 1 - 1e-10) # 1st.method\n",
        "#           a = np.clip(a, -100, None)       # 2st.method\n",
        "\n",
        "            L = np.sum( -( y * np.log(a) + (1 - y) * np.log(1-a) ) )\n",
        "            loss += L\n",
        "                \n",
        "#                 pass # traninig loop (1 epoch)\n",
        "            \n",
        "            # 로지스틱손실함수의 평균값 계산\n",
        "            self.losses.append( ( loss + self.reg_loss() )  / m )\n",
        "\n",
        "            # 검증 세트에 대해서도 손실을 계산\n",
        "            self.update_val_loss(x_val, y_val)\n",
        "            \n",
        "            pass # epoch loop\n",
        "        \n",
        "        pass # fit\n",
        "    \n",
        "    # L1/L2 규제의 손실값 계산\n",
        "    def reg_loss(self):\n",
        "        # L1 norm 의 정의식 대로 계산\n",
        "        # L1 규제의 미분결과 = a x sign(w)\n",
        "        L1_loss = self.l1 * np.sum(np.abs(self.w))\n",
        "        (self.l1 * np.sign(self.w1) + self.l2 * self.w1) / m\n",
        "        # L2 norm 의 정의식 대로 계산\n",
        "        # L2 규제의 미분결과 = a x w\n",
        "        L2_loss = self.l2 / 2 * np.sum(self.w**2)\n",
        "        \n",
        "        return  L1_loss + L2_loss                \n",
        "        pass # reg_loss\n",
        "    \n",
        "    \n",
        "    # 검증 데이터 셋에 대해서도, 동일하게\n",
        "    # 손실함수를 계산하고, L1/L2 규제도 적용하여, 리스트에 저장\n",
        "    def update_val_loss(self, x_val, y_val):        \n",
        "        z = self.forpass(x_val) # 정방향 계산 수행\n",
        "        a = self.activation(z)     # 활성화 함수 통과\n",
        "        a = np.clip(a, 1e-10, 1-1e-10)\n",
        "            \n",
        "        # 검증셋에 대한 손실함수의 값을 계산\n",
        "        # (1) Logistic 손실함수의 값을 계산\n",
        "        val_loss = np.sum(-(y_val * np.log(a) + (1-y_val) * np.log(1-a)))\n",
        "                    \n",
        "        # 검증셋의 평균손실값을 계산하여, 파이썬 리스트(val_losses)에 추가\n",
        "        # 더불어서, L1/L2 규제의 손실값을 계산하여 추가\n",
        "        self.val_losses.append( \n",
        "            ( val_loss + self.reg_loss() ) / len(y_val)\n",
        "        )\n",
        "        \n",
        "        pass # update_val_loss\n",
        "    \n",
        "    \n",
        "    # 분류예측함수\n",
        "    def predict(self, x):\n",
        "        # 정방향계싼 수행\n",
        "        z = self.forpass(x)\n",
        "        \n",
        "        # 간결성과 가독성을 위해서는, 죽~아래코드를 그대로 활용\n",
        "        # if z > 0, sigmoid(z) > 0.5\n",
        "        # if z < 0, sigmoid(z) < 0.5\n",
        "#         return np.array(z) > 0 # 1-D, Vector\n",
        "        \n",
        "        # 활성화함수 통과\n",
        "        # np.array(z) : python list -> ndarray (1-D, Vector) 변환\n",
        "        a = self.activation(np.array(z))  \n",
        "        \n",
        "        # 임계함수로 step function 통과시켜, 최종 분류예측값 산출\n",
        "        return a > .5\n",
        "        pass    # predict\n",
        "    \n",
        "    # 모델의 성능(정확도)평가 함수\n",
        "    def score(self, x, y):\n",
        "        return np.mean(self.predict(x) == y.reshape(-1, 1))\n",
        "        pass # score\n",
        "\n",
        "    pass # end class"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J-2PbLwbGo69",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DualLayer(SingleLayer):\n",
        "\n",
        "    # 각종 하이퍼 파아미터와 가중치와 절편 행렬의 초기화\n",
        "    def __init__(self, units = 10, learning_rate = .1, l1 = 0, l2 = 0):\n",
        "        self.units = units\n",
        "\n",
        "        # 은닉층의 가중치(w)와 절편(b)를 초기화\n",
        "        self.w1 = None\n",
        "        self.b1 = None\n",
        "\n",
        "        # 출력층의 가중치(w)와 절편(b)를 초기화\n",
        "        self.w2 = None\n",
        "        self.b2 = None\n",
        "\n",
        "        self.losses = []\n",
        "        self.val_losses = []\n",
        "\n",
        "        self.lr = learning_rate     # eta\n",
        "\n",
        "        self.l1 = l1\n",
        "        self.l2 = l2\n",
        "\n",
        "        self.a1 = None\n",
        "        pass    # constructor\n",
        "    \n",
        "    # 정방향 계산\n",
        "    def forpass(self, x):\n",
        "        \n",
        "        # 은닉층의 선형함수 출력값 계산\n",
        "        z1 = np.dot(x, self.w1) + self.b1\n",
        "        self.a1 = self.activation(z1)       # 활성화\n",
        "\n",
        "        # 출력층의 선형함수 출력값 계산\n",
        "        z2 = np.dot(self.a1, self.w2) + self.b1\n",
        "\n",
        "        return z2\n",
        "        pass    # forpass\n",
        "\n",
        "    # 역전파된 오차를 가지고 입력층 이전 계층('은닉층')까지의 모든 계층의\n",
        "    # 가중치와 절편의 '평균' 변화율을 계산 및 반환\n",
        "    def backprop(self, x, err):     # x, err: 각각 다 행렬\n",
        "        m = len(x)      # 전체 샘플의 크기 확보\n",
        "\n",
        "        # '출력층'의 가중치(w)와 절편(b)의 평균 변화율 계산\n",
        "        w2_grad = np.dot(self.a1.T, err) / m\n",
        "        b2_grad = np.sum(err) / m\n",
        "\n",
        "        # '은닉층'의 가중치(w1)와 절편(b1)의 평균 변화율 계산\n",
        "        temp = np.dot(self.w1.T, err) * self.a1 * (1 - self.a1)\n",
        "\n",
        "        w1_grad = np.dot(x.T, temp) / m\n",
        "        b1_grad = np.sum(temp, axis=0) / m\n",
        "        pass        # backprop\n",
        "\n",
        "    # 신경망 각 층의 가중치와 절편을 초기화\n",
        "    def init_weights(self, n_features):\n",
        "        # 은닉층의 가중치와 절편 초기화\n",
        "        self.w1 = np.ones( (n_features, self.units) )\n",
        "        self.b1 = np.zeros(self.units)\n",
        "\n",
        "        # 출력층의 가중치와 절편 초기화\n",
        "        self.w2 = np.ones((self.units, 1))\n",
        "        self.b2 = 0\n",
        "\n",
        "        pass        # init_weights\n",
        "\n",
        "    # 신경망의 모델을 훈련시키는 핵심 함수\n",
        "    def training(self, x, y, m):\n",
        "        # 1. 정방향 계산 수행\n",
        "        z = self.forpass(x)\n",
        "\n",
        "        # 2. 활성화 출력값 계산 수행\n",
        "        a = self.activation(z)\n",
        "\n",
        "        # 3. 오차 계산\n",
        "        err = -(y - a)\n",
        "\n",
        "        # 4. 오차 역전파를 통한 각 층의 가중치와 절편 변화율 계산\n",
        "        w1_grad, b1_grad, w2_grad, b2_grad = self.backprop(x, err)\n",
        "\n",
        "        # 5. L1/L2 규제의 미분값을 가중치의 변화율에 더하고 평균 계산\n",
        "        w1_grad += (self.l1 * np.sign(self.w1) + self.l2 * self.w1) / m\n",
        "        w2_grad += (self.l1 * np.sign(self.w2) + self.l2 * self.w2) / m\n",
        "        \n",
        "        # 6. '은닉층', '출력층'의 가중치와 절편을 업데이트(1 epoch마다)\n",
        "        self.w1 -= self.lr * w1_grad\n",
        "        self.b1 -= self.lr * b1_grad\n",
        "\n",
        "        self.w2 -= self.lr * w2_grad\n",
        "        self.b2 -= self.lr * b2_grad\n",
        "\n",
        "        return a\n",
        "        pass        # training\n",
        "\n",
        "    # '은닉층'과 '출력층'의 L1/L2 규제 손실 계산\n",
        "    def reg_loss(self):\n",
        "\n",
        "        return self.l1 * ( np.sum( np.abs(self.w1) ) ) + np.sum( np.abs(self.w2) ) + self.l2 / 2 * ( np.sum(self.w1**2) + np.sum(self.w2**2) )\n",
        "        pass # reg_loss\n",
        "\n",
        "\n",
        "    def fit(self, x, y, epochs=100, x_val=None, y_val=None):\n",
        "        # 분석가가 제공한 정답(훈련셋/검증셋의 정답)을\n",
        "        # 열 벡터 모양의 행렬로 변환\n",
        "        y = y.reshape(-1, 1)\n",
        "\n",
        "        # 전체샘플크기 확보\n",
        "        m = len(x)\n",
        "\n",
        "        #  가중치와 절편의 최초 초기화 수행\n",
        "        self.init_weights(x.shape[1])\n",
        "\n",
        "        # epoch loop\n",
        "        for i in range(epochs):\n",
        "            # 정방향 계산, 오차역전파, 역방향계산,\n",
        "            # L1/L2 규제 적용 등을 통해, 각 계층의\n",
        "            # 가중치와 절편의 변화율을 구하고 업데이트 수행\n",
        "            a = self.training(x, y, m) # *******\n",
        "            a = np.clip(a, 1e-10, 1-1e-10)\n",
        "            # L (손실함수)의 손실과 L1/L2 규제 손실을 계산하여\n",
        "            # 파이썬 리스트에 추가\n",
        "            loss = np.sum( -( y * np.log(a) + (1-y) * np.log(1-a) ) )\n",
        "            L_1_2 = self.reg_loss()\n",
        "\n",
        "            self.losses.append( ( loss + L_1_2 ) / m )\n",
        "\n",
        "            # 검증셋에 대한 손실값 계산 후 파이썬 리스트에 추가\n",
        "            self.update_val_loss(x_val, y_val)\n",
        "            pass        # epoch loop\n",
        "\n",
        "        pass        # fit"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n6MwJNzpyUWm",
        "colab_type": "code",
        "outputId": "93b8f0d2-8c58-4b3f-a519-1099a98fecee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "dualNN = DualLayer(l2 = .01)\n",
        "dualNN"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<__main__.DualLayer at 0x7f1879400fd0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jAJXLLXW0Cpy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "cancer = load_breast_cancer()\n",
        "x = cancer.data\n",
        "y = cancer.target\n",
        "\n",
        "# x.shape, y.shape\n",
        "\n",
        "# (1-1) 1단계 분할 : 8:2 로, 훈련:테스트로 분할\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "x_train_all, x_test_all, y_train_all, y_test_all = \\\n",
        "    train_test_split(x, y, test_size=.2, stratify=y)\n",
        "\n",
        "# (1-2) 2단계 분할 : 1단계의 분할된 훈련 데이터 셋을, 다시\n",
        "#                    8:2 로, 훈련:검증로 분할\n",
        "x_train, x_val, y_train, y_val = \\\n",
        "    train_test_split(\n",
        "        x_train_all, \n",
        "        y_train_all, \n",
        "        test_size=.2,\n",
        "        stratify=y_train_all\n",
        "    )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DV-rwhVF0YMA",
        "colab_type": "code",
        "outputId": "2ab65f5e-81ca-4921-bd81-1fd6a554c37f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# 입력데이터에 대한 표준화 전처리 수행 - 2st. method (직접구현)\n",
        "x_train.shape\n",
        "\n",
        "# 표준화 전처리 수행\n",
        "# (1) 평균 산출 > (2) 표준편차 산출 > (3) 공식에 따라 표준화 수행\n",
        "\n",
        "# 30개의 각 특성별 평균을 구하라!!\n",
        "x_train_mean = np.mean(x_train, axis=0) # 평균\n",
        "\n",
        "# 30개의 각 특성별 표준편차를 구하라!!!\n",
        "x_train_std = np.std(x_train, axis=0)   # 표준편차\n",
        "\n",
        "x_train_scaled = ( x_train - x_train_mean ) / x_train_std\n",
        "x_train_scaled.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(364, 30)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JuHQPcC60dSp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 여러 셋(훈련/검증/테스트)으로 나누어진, 각각의 셋을 표준화시킬때에는\n",
        "# 훈련셋의 통계량(평균, 표준편차)를 기준으로 나머지 검증/테스트 셋도\n",
        "# 표준화 시켜야 함\n",
        "\n",
        "# x_train_mean ---> 훈련셋의 평균\n",
        "# x_train_std  ---> 훈련셋의 표준편차\n",
        "\n",
        "x_val_scaled = (x_val - x_train_mean) / x_train_std\n",
        "# x_test_scaled = (x_test - x_train_mean ) / x_train_std"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UzyMgKdB0fie",
        "colab_type": "code",
        "outputId": "a4e865eb-9253-4903-d1c9-6f05dd358006",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "x_train_scaled.shape, x_val_scaled.shape, y_train.shape, y_val.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((364, 30), (91, 30), (364,), (91,))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OqyjKpbB0hfW",
        "colab_type": "code",
        "outputId": "8f0d4816-3285-4ddf-badd-179541af5b54",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 369
        }
      },
      "source": [
        "dualNN.fit(x_train_scaled, y_train, x_val = x_val_scaled, y_val=y_val, epochs = 20000)\n",
        "dualNN.score(x_val_scaled, y_val)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-30-57aac8bb4a7c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdualNN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train_scaled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx_val_scaled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m20000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdualNN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_val_scaled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-28-84c816d176a8>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, epochs, x_val, y_val)\u001b[0m\n\u001b[1;32m    116\u001b[0m             \u001b[0;31m# L1/L2 규제 적용 등을 통해, 각 계층의\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m             \u001b[0;31m# 가중치와 절편의 변화율을 구하고 업데이트 수행\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m             \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# *******\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m             \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1e-10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1e-10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0;31m# L (손실함수)의 손실과 L1/L2 규제 손실을 계산하여\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-28-84c816d176a8>\u001b[0m in \u001b[0;36mtraining\u001b[0;34m(self, x, y, m)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0;31m# 4. 오차 역전파를 통한 각 층의 가중치와 절편 변화율 계산\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m         \u001b[0mw1_grad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb1_grad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw2_grad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb2_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackprop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0;31m# 5. L1/L2 규제의 미분값을 가중치의 변화율에 더하고 평균 계산\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-28-84c816d176a8>\u001b[0m in \u001b[0;36mbackprop\u001b[0;34m(self, x, err)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;31m# '은닉층'의 가중치(w1)와 절편(b1)의 평균 변화율 계산\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m         \u001b[0mtemp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mw1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ma1\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ma1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0mw1_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemp\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mdot\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: shapes (10,30) and (364,10) not aligned: 30 (dim 1) != 364 (dim 0)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u0pTEuqI04vh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}